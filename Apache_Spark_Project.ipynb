{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Apache Spark  Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZQdJqAKfhVNO",
        "GqkIsKnitZRw"
      ],
      "authorship_tag": "ABX9TyONKGqTzW3QlhshINHOfgjk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luciekash/Apache-Spark-Project-/blob/main/Apache_Spark_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apache Spark DataFrames Project\n",
        "\n",
        "**1.Problem Statement**\n",
        "\n",
        "As a Data professional, you need to perform an analysis by answering questions about some stock market data on Safaricom from the years 2012-2017.\n",
        "\n",
        "You will need to perform the following:\n",
        "\n",
        "**Data Importation and Exploration**\n",
        "\n",
        "● Start a spark session and load the stock file while inferring the data types.\n",
        "\n",
        "● Determine the column names\n",
        "\n",
        "● Make observations about the schema.\n",
        "\n",
        "● Show the first 5 rows\n",
        "\n",
        "● Use the describe method to learn about the data frame\n",
        "\n",
        "**Data Preparation**\n",
        "\n",
        "● Format all the data to 2 decimal places i.e. format_number()\n",
        "\n",
        "● Create a new data frame with a column called HV Ratio that is the ratio of the High Price versus volume of stock traded for a day\n",
        "\n",
        "\n",
        "\n",
        "**Data Analysis**\n",
        "\n",
        "● What day had the Peak High in Price?\n",
        "\n",
        "● What is the mean of the Close column?\n",
        "\n",
        "● What is the max and min of the Volume column?\n",
        "\n",
        "● How many days was the Close lower than 60 dollars?\n",
        "\n",
        "● What percentage of the time was the High greater than 80 dollars?\n",
        "\n",
        "● What is the Pearson correlation between High and Volume?\n",
        "\n",
        "● What is the max High per year?\n",
        "\n",
        "● What is the average Close for each Calendar Month?"
      ],
      "metadata": {
        "id": "0eqcYKzLaB9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Importation and Exploration"
      ],
      "metadata": {
        "id": "i2JBHiTdZSEM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heu-Cd3uZONc",
        "outputId": "481cc49c-5078-4a60-fd0e-9645dbfc9c74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 43 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 37.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=8d1693ba28ac2d2fd75c7b59223da151418e960318a090335ca29a218a4137bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ],
      "source": [
        "# Installing pyspark\n",
        "# ---\n",
        "#\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we run a local spark session\n",
        "# ---\n",
        "#\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from pyspark.sql import SQLContext\n",
        "sqlCtx = SQLContext(sc)"
      ],
      "metadata": {
        "id": "hl-7b-00ZcFp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read csv and display schema to determine the column names\n",
        "df = spark.read.csv(\"saf_stock.csv\", header=True)\n",
        "print(df.columns)\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VO-h9_b1bP-x",
        "outputId": "9cc0b17e-2183-4dd4-d8a6-255758bd57a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n",
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Open: string (nullable = true)\n",
            " |-- High: string (nullable = true)\n",
            " |-- Low: string (nullable = true)\n",
            " |-- Close: string (nullable = true)\n",
            " |-- Volume: string (nullable = true)\n",
            " |-- Adj Close: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations:\n",
        "\n",
        "There are 7 columns\n",
        "\n",
        "Columns are nullable\n",
        "\n",
        "Column type is String\n",
        "\n",
        "The first row is a header that contains the column names"
      ],
      "metadata": {
        "id": "DBNTHwoEgFR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Show the first 5 rows\n",
        "\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRJuD_5IgO2u",
        "outputId": "406de198-9155-415b-f437-0a74d8cb7a3e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+---------+---------+---------+--------+---------+\n",
            "|    Date|     Open|     High|      Low|    Close|  Volume|Adj Close|\n",
            "+--------+---------+---------+---------+---------+--------+---------+\n",
            "|1/3/2012|59.970001|61.060001|59.869999|60.330002|12668800|52.619235|\n",
            "|1/4/2012|60.209999|60.349998|59.470001|59.709999| 9593300|52.078475|\n",
            "|1/5/2012|59.349998|59.619999|58.369999|59.419998|12768200|51.825539|\n",
            "|1/6/2012|59.419998|59.450001|58.869999|       59| 8069400| 51.45922|\n",
            "|1/9/2012|59.029999|59.549999|58.919998|    59.18| 6679300|51.616215|\n",
            "+--------+---------+---------+---------+---------+--------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the describe method to learn about the data frame\n",
        "df.describe().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r1yb02lgmDe",
        "outputId": "70ee86e8-5dd2-409b-8843-7d142a4816fa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|summary|     Date|             Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
            "+-------+---------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|  count|     1258|             1258|             1258|             1258|             1258|             1258|             1258|\n",
            "|   mean|     null|72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
            "| stddev|     null| 6.76809024470826|6.768186808159218|6.744075756255496| 6.75685916373299|  4519780.8431556|6.722609449996858|\n",
            "|    min|1/10/2012|        56.389999|        57.060001|        56.299999|        56.419998|         10010500|        50.363689|\n",
            "|    max| 9/9/2016|        90.800003|        90.970001|            89.25|        90.470001|          9994400|        84.914216|\n",
            "+-------+---------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Data Preparation"
      ],
      "metadata": {
        "id": "ZQdJqAKfhVNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Format all the data to 2 decimal places\n",
        "\n",
        "from pyspark.sql.functions import format_number\n",
        "\n",
        "df_formatted=df.select(df['Date'],\n",
        "              format_number(df['Open'].cast('float'),2).alias('Open'),\n",
        "              format_number(df['High'].cast('float'), 2).alias('High'),\n",
        "              format_number(df['Low'].cast('float'), 2).alias('Low'),\n",
        "              format_number(df['Close'].cast('float'), 2).alias('Close'),\n",
        "              df['Volume'].cast('int').alias('Volume'),\n",
        "              format_number(df['Adj Close'].cast('float'), 2).alias('Adj Close')\n",
        "              )\n",
        "\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "df_formatted.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsP7YkVBhnKX",
        "outputId": "f963319a-f8fe-41dc-e40b-f80e7625adb1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+-----+-----+-----+--------+---------+\n",
            "|     Date| Open| High|  Low|Close|  Volume|Adj Close|\n",
            "+---------+-----+-----+-----+-----+--------+---------+\n",
            "| 1/3/2012|59.97|61.06|59.87|60.33|12668800|    52.62|\n",
            "| 1/4/2012|60.21|60.35|59.47|59.71| 9593300|    52.08|\n",
            "| 1/5/2012|59.35|59.62|58.37|59.42|12768200|    51.83|\n",
            "| 1/6/2012|59.42|59.45|58.87|59.00| 8069400|    51.46|\n",
            "| 1/9/2012|59.03|59.55|58.92|59.18| 6679300|    51.62|\n",
            "|1/10/2012|59.43|59.71|58.98|59.04| 6907300|    51.49|\n",
            "|1/11/2012|59.06|59.53|59.04|59.40| 6365600|    51.81|\n",
            "|1/12/2012|59.79|60.00|59.40|59.50| 7236400|    51.90|\n",
            "|1/13/2012|59.18|59.61|59.01|59.54| 7729300|    51.93|\n",
            "|1/17/2012|59.87|60.11|59.52|59.85| 8500000|    52.20|\n",
            "|1/18/2012|59.79|60.03|59.65|60.01| 5911400|    52.34|\n",
            "|1/19/2012|59.93|60.73|59.75|60.61| 9234600|    52.86|\n",
            "|1/20/2012|60.75|61.25|60.67|61.01|10378800|    53.21|\n",
            "|1/23/2012|60.81|60.98|60.51|60.91| 7134100|    53.13|\n",
            "|1/24/2012|60.75|62.00|60.75|61.39| 7362800|    53.54|\n",
            "|1/25/2012|61.18|61.61|61.04|61.47| 5915800|    53.61|\n",
            "|1/26/2012|61.80|61.84|60.77|60.97| 7436200|    53.18|\n",
            "|1/27/2012|60.86|61.12|60.54|60.71| 6287300|    52.95|\n",
            "|1/30/2012|60.47|61.32|60.35|61.30| 7636900|    53.47|\n",
            "|1/31/2012|61.53|61.57|60.58|61.36| 9761500|    53.52|\n",
            "+---------+-----+-----+-----+-----+--------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new data frame with a column called HV Ratio that is the ratio of the High Price versus volume of stock traded for a day\n",
        "df_new = df_formatted.withColumn('HV Ratio', df_formatted['High']/ df_formatted['Volume'])\n",
        "df_new.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kztqwbN5o57-",
        "outputId": "605cd27f-1d62-40b4-8017-7386690730dc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+-----+-----+-----+--------+---------+--------------------+\n",
            "|     Date| Open| High|  Low|Close|  Volume|Adj Close|            HV Ratio|\n",
            "+---------+-----+-----+-----+-----+--------+---------+--------------------+\n",
            "| 1/3/2012|59.97|61.06|59.87|60.33|12668800|    52.62|4.819714574387472E-6|\n",
            "| 1/4/2012|60.21|60.35|59.47|59.71| 9593300|    52.08|6.290848821573389...|\n",
            "| 1/5/2012|59.35|59.62|58.37|59.42|12768200|    51.83|4.669413073103491E-6|\n",
            "| 1/6/2012|59.42|59.45|58.87|59.00| 8069400|    51.46|7.367338339901356E-6|\n",
            "| 1/9/2012|59.03|59.55|58.92|59.18| 6679300|    51.62|8.915604928660188E-6|\n",
            "|1/10/2012|59.43|59.71|58.98|59.04| 6907300|    51.49|8.644477581688938E-6|\n",
            "|1/11/2012|59.06|59.53|59.04|59.40| 6365600|    51.81| 9.35182857861003E-6|\n",
            "|1/12/2012|59.79|60.00|59.40|59.50| 7236400|    51.90| 8.29141562102703E-6|\n",
            "|1/13/2012|59.18|59.61|59.01|59.54| 7729300|    51.93|7.712211972623653E-6|\n",
            "|1/17/2012|59.87|60.11|59.52|59.85| 8500000|    52.20|7.071764705882352...|\n",
            "|1/18/2012|59.79|60.03|59.65|60.01| 5911400|    52.34|1.015495483303447...|\n",
            "|1/19/2012|59.93|60.73|59.75|60.61| 9234600|    52.86|6.576354146362592...|\n",
            "|1/20/2012|60.75|61.25|60.67|61.01|10378800|    53.21| 5.90145296180676E-6|\n",
            "|1/23/2012|60.81|60.98|60.51|60.91| 7134100|    53.13|8.547679455011844E-6|\n",
            "|1/24/2012|60.75|62.00|60.75|61.39| 7362800|    53.54|8.420709512685392E-6|\n",
            "|1/25/2012|61.18|61.61|61.04|61.47| 5915800|    53.61|1.041448324825044...|\n",
            "|1/26/2012|61.80|61.84|60.77|60.97| 7436200|    53.18|8.316075414862431E-6|\n",
            "|1/27/2012|60.86|61.12|60.54|60.71| 6287300|    52.95|9.721183974042911E-6|\n",
            "|1/30/2012|60.47|61.32|60.35|61.30| 7636900|    53.47|8.029436027707578E-6|\n",
            "|1/31/2012|61.53|61.57|60.58|61.36| 9761500|    53.52|6.307432259386365E-6|\n",
            "+---------+-----+-----+-----+-----+--------+---------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Data Analysis"
      ],
      "metadata": {
        "id": "GqkIsKnitZRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#What day had the Peak High in Price?\n",
        "print(df_new.orderBy(df_new['High'].desc()).head(1)[0][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILfVmMqutdVC",
        "outputId": "152f73d2-2097-47e1-dcbc-ceb885d6385c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/13/2015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#What is the mean of the Close column?\n",
        "from pyspark.sql.functions import mean, max, min\n",
        "\n",
        "print(df_new.select(mean('Close')).show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5qWcPlgvLSq",
        "outputId": "0b57263a-b1ef-4d63-aa04-422ceab242f3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|       avg(Close)|\n",
            "+-----------------+\n",
            "|72.38844992050863|\n",
            "+-----------------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#What is the max and min of the Volume column?\n",
        "print(df_new.select(max('Volume') ,min('Volume')).show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWkMpby5wCGw",
        "outputId": "5289a661-014b-43aa-fafc-8bd0d30e6eab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "|max(Volume)|min(Volume)|\n",
            "+-----------+-----------+\n",
            "|   80898100|    2094900|\n",
            "+-----------+-----------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#How many days was the Close lower than 60 dollars?\n",
        "print(df_new.filter(df_new['Close'] < 60).count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfUtpcdew3HC",
        "outputId": "b7b17655-6280-46d4-9fd2-31b59502f2c3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#What percentage of the time was the High greater than 80 dollars\n",
        "\n",
        "print((df_new.filter(df_new['High']>80).count()/df.count()) * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98bAfLSUyGUB",
        "outputId": "8a3fbe7b-2877-4b35-b165-6282a9205ba7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.426073131955485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#What is the Pearson correlation between High and Volume?\n",
        "from pyspark.sql.functions import corr\n",
        "print(df_new.select(corr('High','Volume')).show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4iaEn4OzMZA",
        "outputId": "55b5c3ee-e78e-41f8-d57d-1f9cad3af2a6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|  corr(High, Volume)|\n",
            "+--------------------+\n",
            "|-0.33843260582148915|\n",
            "+--------------------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the max High per year?\n",
        "\n",
        "from pyspark.sql.functions import year\n",
        "\n",
        "df_new_2=df_new.select(df_new['Date'],\n",
        "              df_new['High'].cast('float').alias('High'),\n",
        "              df_new['Close'].cast('float').alias('Close')\n",
        "              )\n",
        "\n",
        "df_year = df_new_2.withColumn(\"Year\",year(df_new_2['Date']))\n",
        "maxim= df_year.groupBy('Year').max()\n",
        "print(maxim.select('Year','max(High)').orderBy('Year').show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J18hwIF2T9C",
        "outputId": "f3ed5da7-7dc0-4a42-ae12-a46b5e7e6ceb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|Year|max(High)|\n",
            "+----+---------+\n",
            "|null|    90.97|\n",
            "+----+---------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import month\n",
        "\n",
        "df_month = df_new_2.withColumn('Month',month('Date'))\n",
        "avg_month = df_month.select(['Month','Close']).groupBy('Month').mean()\n",
        "print(avg_month.select('Month','avg(Close)').orderBy('Month').show())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkG9Msey2iwi",
        "outputId": "d22c111d-f550-48e3-dc2a-ed49539e05df"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------+\n",
            "|Month|       avg(Close)|\n",
            "+-----+-----------------+\n",
            "| null|72.38844997363553|\n",
            "+-----+-----------------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}